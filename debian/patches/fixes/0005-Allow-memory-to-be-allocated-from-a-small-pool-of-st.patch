From 1ecbeecbdb39699e44db3d9e1d92c4d7893f8103 Mon Sep 17 00:00:00 2001
From: Mike Hommey <glandium@debian.org>
Date: Sun, 29 Jun 2008 14:36:53 +0200
Subject: [PATCH 05/30] Allow memory to be allocated from a small pool of static memory during init

Some LD_PRELOADed libraries such as libaoss and libtrash can divert some
functions used by jemalloc initialization, themselves needing malloc or
calloc. This used to lead to a deadlock.
---
 memory/jemalloc/jemalloc.c |   38 ++++++++++++++++++++++++++++++++++++++
 1 files changed, 38 insertions(+), 0 deletions(-)

diff --git a/memory/jemalloc/jemalloc.c b/memory/jemalloc/jemalloc.c
index d5e01f8..8415ab4 100644
--- a/memory/jemalloc/jemalloc.c
+++ b/memory/jemalloc/jemalloc.c
@@ -621,6 +621,7 @@ typedef malloc_spinlock_t malloc_mutex_t;
 
 /* Set to true once the allocator has been initialized. */
 static bool malloc_initialized = false;
+static bool malloc_initializing = false;
 
 #if defined(MOZ_MEMORY_WINDOWS)
 /* No init lock for Windows. */
@@ -3796,9 +3797,27 @@ arena_malloc(arena_t *arena, size_t size, bool zero)
 		return (arena_malloc_large(arena, size, zero));
 }
 
+#define STATIC_BUFFER_SIZE (64 * 1024)
+static char static_buffer[STATIC_BUFFER_SIZE];
+
+static inline void *
+static_alloc(size_t size)
+{
+	static char *next_buf = static_buffer;
+	char *ret = next_buf;
+
+	next_buf = &next_buf[size];
+	if (next_buf <= &static_buffer[STATIC_BUFFER_SIZE])
+		return ret;
+
+	return NULL;
+}
+
 static inline void *
 imalloc(size_t size)
 {
+	if (malloc_initializing)
+		goto static_alloc;
 
 	assert(size != 0);
 
@@ -3806,16 +3825,22 @@ imalloc(size_t size)
 		return (arena_malloc(choose_arena(), size, false));
 	else
 		return (huge_malloc(size, false));
+static_alloc:
+	return static_alloc(size);
 }
 
 static inline void *
 icalloc(size_t size)
 {
+	if (malloc_initializing)
+		goto static_alloc;
 
 	if (size <= arena_maxclass)
 		return (arena_malloc(choose_arena(), size, true));
 	else
 		return (huge_malloc(size, true));
+static_alloc:
+	return static_alloc(size);
 }
 
 /* Only handles large allocations that require more than page alignment. */
@@ -4236,11 +4261,16 @@ idalloc(void *ptr)
 
 	assert(ptr != NULL);
 
+	if ((ptr >= (void *)static_buffer) && (ptr < (void *)&static_buffer[STATIC_BUFFER_SIZE]))
+		goto end;
+
 	chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);
 	if (chunk != ptr)
 		arena_dalloc(chunk->arena, chunk, ptr);
 	else
 		huge_dalloc(ptr);
+end:
+	return;
 }
 
 static void
@@ -5196,6 +5226,10 @@ malloc_init_hard(void)
 	int linklen;
 #endif
 
+	/* Recursive malloc_init(), it means we still need to allocate memory */
+	if (malloc_initializing)
+		return(false);
+
 #ifndef MOZ_MEMORY_WINDOWS
 	malloc_mutex_lock(&init_lock);
 #endif
@@ -5210,6 +5244,7 @@ malloc_init_hard(void)
 #endif
 		return (false);
 	}
+	malloc_initializing = true;
 
 #ifdef MOZ_MEMORY_WINDOWS
 	/* get a thread local storage index */
@@ -5662,6 +5697,7 @@ MALLOC_OUT:
 	/* Allocate and initialize arenas. */
 	arenas = (arena_t **)base_alloc(sizeof(arena_t *) * narenas);
 	if (arenas == NULL) {
+		malloc_initializing = false;
 #ifndef MOZ_MEMORY_WINDOWS
 		malloc_mutex_unlock(&init_lock);
 #endif
@@ -5679,6 +5715,7 @@ MALLOC_OUT:
 	 */
 	arenas_extend(0);
 	if (arenas[0] == NULL) {
+		malloc_initializing = false;
 #ifndef MOZ_MEMORY_WINDOWS
 		malloc_mutex_unlock(&init_lock);
 #endif
@@ -5713,6 +5750,7 @@ MALLOC_OUT:
 		return (true);
 #endif
 
+	malloc_initializing = false;
 	malloc_initialized = true;
 #ifndef MOZ_MEMORY_WINDOWS
 	malloc_mutex_unlock(&init_lock);
-- 
1.7.0.rc0.69.g2ee80

